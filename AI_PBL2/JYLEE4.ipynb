{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loosely inspired by http://abel.ee.ucla.edu/cvxopt/_downloads/mnist.py\n",
    "which is GPL licensed.\n",
    "\"\"\"\n",
    "\n",
    "def read(dataset = \"training\", path = \".\"):\n",
    "    \"\"\"\n",
    "    Python function for importing the MNIST data set.  It returns an iterator\n",
    "    of 2-tuples with the first element being the label and the second element\n",
    "    being a numpy.uint8 2D array of pixel data for the given image.\n",
    "    \"\"\"\n",
    "\n",
    "    if dataset is \"training\":\n",
    "        fname_img = os.path.join(path, 'newtrain-images-idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 'newtrain-labels-idx1-ubyte')\n",
    "    elif dataset is \"testing\":\n",
    "        fname_img = os.path.join(path, 'mnist-new1k-images-idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 'mnist-new1k-labels-idx1-ubyte')\n",
    "    else:\n",
    "        raise Exception(\"dataset must be 'testing' or 'training'\")\n",
    "\n",
    "    # Load everything in some numpy arrays\n",
    "    with open(fname_lbl, 'rb') as flbl:\n",
    "        magic, num = struct.unpack(\">II\", flbl.read(8))\n",
    "        lbl = np.fromfile(flbl, dtype=np.int8)\n",
    "\n",
    "    with open(fname_img, 'rb') as fimg:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "        img = np.fromfile(fimg, dtype=np.uint8).reshape(len(lbl), rows, cols)\n",
    "\n",
    "    get_img = lambda idx: (lbl[idx], img[idx])\n",
    "\n",
    "    # Create an iterator which returns each image in turn\n",
    "    for i in range(len(lbl)):\n",
    "        yield get_img(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 28, 28)\n",
      "(80000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "tr = list(read(\"training\", \"newdata/\"))\n",
    "ts = list(read(\"testing\", 'newdata/'))\n",
    "\n",
    "images_train = np.array(list(zip(*tr))[1])\n",
    "labels_train = np.array(list(zip(*tr))[0])\n",
    "\n",
    "images_test = np.array(list(zip(*ts))[1])\n",
    "labels_test = np.array(list(zip(*ts))[0])\n",
    "\n",
    "print(images_train.shape)\n",
    "print(labels_train.shape)\n",
    "print(images_test.shape)\n",
    "print(labels_test.shape)\n",
    "\n",
    "y_train = labels_train\n",
    "y_test = labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, Y, model, default_param, param_grid, cv, save, filename):\n",
    "    \"\"\"\n",
    "    Train model using given parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    ------------------------\n",
    "    - X: training data images\n",
    "    - Y: training data labels\n",
    "    - model: estimator class (NOT object)\n",
    "    - default_param: default parameters you want to apply to model\n",
    "    - param_grid: used by GridSearchCV\n",
    "    - cv: k in K-Fold. cross-validation parameter\n",
    "    - save: whether you save trained model or not\n",
    "    - filename: path of file to save your model\n",
    "    \n",
    "    Returns:\n",
    "    ------------------------\n",
    "    - estimator: trained model.\n",
    "    \"\"\"\n",
    "    \n",
    "    clf = model(**default_param)\n",
    "    estimator = GridSearchCV(clf, param_grid, cv=cv, n_jobs=4)\n",
    "    \n",
    "    estimator.fit(X, Y)\n",
    "    \n",
    "    if save:\n",
    "        joblib.dump(estimator, filename)\n",
    "        \n",
    "    return estimator\n",
    "\n",
    "def get_model(model, filename, param_grid=dict(), cv=6, X=None, Y=None, default_param=dict(), force_training=False, save=True):\n",
    "    \"\"\"\n",
    "    get model from existing file which has pre-trained model\n",
    "    or create new model.\n",
    "    \n",
    "    Arguments:\n",
    "    --------------------------\n",
    "    - model: class of estimator\n",
    "    - filename: name of file which contains pre-trained model.\n",
    "    - default_param: default parameters you want to apply to model\n",
    "    - param_grid: parameters you want to pass to GridSearchCV as parameters of 'param_grid'\n",
    "    - cv: cross-validation parameter\n",
    "    - X: training data images\n",
    "    - Y: training data labels\n",
    "    - forece_training: if you want this function to ONLY train not load from file, set True.\n",
    "    - save: if you want to save model after training, set this True.\n",
    "    \n",
    "    Returns:\n",
    "    --------------------------\n",
    "    - estimator: model trained.\n",
    "    \"\"\"\n",
    "    \n",
    "    path = pathlib.Path(filename)\n",
    "    estimator = None\n",
    "    \n",
    "    if force_training == True:\n",
    "        estimator = train_model(X, Y, model, default_param, param_grid, cv, save, filename)\n",
    "    else:\n",
    "        if path.exists(): # if file exists, just load that.\n",
    "            estimator = joblib.load(filename)\n",
    "        else:\n",
    "            estimator = train_model(X, Y, model, default_param, param_grid, cv, save, filename)\n",
    "            \n",
    "    return estimator\n",
    "\n",
    "def get_model_without_gridsearch(model, filename, default_param=dict(), X=None, Y=None, save=True):\n",
    "    \"\"\"\n",
    "    Another version of get model without GridSearchCV\n",
    "    \n",
    "    Arguments:\n",
    "    -----------------------------\n",
    "    model: class name of estimator\n",
    "    filename: name of file to store our model into.\n",
    "    default_param: default parameter you want to pass into model.\n",
    "    X: features of training dataset\n",
    "    Y: labels of training dataset\n",
    "    save: if you want to save model after training, set it to True. default: True\n",
    "    \n",
    "    Returns:\n",
    "    -----------------------------\n",
    "    estimator: model which is trained already.\n",
    "    \"\"\"\n",
    "    \n",
    "    if pathlib.Path(filename).exists():\n",
    "        estimator = joblib.load(filename)\n",
    "    else:\n",
    "        estimator = model(**default_param)\n",
    "        estimator.fit(X, Y)\n",
    "        if save:\n",
    "            joblib.dump(estimator, filename)\n",
    "        \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM implementation using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySVM(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    1 vs 1 SVM (binary classification)\n",
    "    \"\"\"\n",
    "    def __init__(self, C=0.1, eta=0.001, batch_size=1, max_iter=25, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        constructor of MyBinarySVM class.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.eta = eta\n",
    "        self.C = C\n",
    "        self.epsilon = epsilon\n",
    "        self.num_classes = 0\n",
    "#         self.beta1 = 0.9\n",
    "#         self.beta2 = 0.99\n",
    "    \n",
    "    def fit(self, X, y=None, params=None):\n",
    "        \"\"\"\n",
    "        fit method for training svm\n",
    "        \n",
    "        Arguments:\n",
    "        --------------------------\n",
    "        X: image data. (60000, 784)\n",
    "        y: label data. (60000, 1)\n",
    "        \n",
    "        Returns:\n",
    "        --------------------------\n",
    "        Z: class score\n",
    "        \"\"\"\n",
    "\n",
    "        m = np.shape(X)[0]\n",
    "        n = np.shape(X)[1]\n",
    "        self.num_classes = len(np.unique(y))\n",
    "        \n",
    "        y_encoded = self.encode_y(y)\n",
    "        \n",
    "        # create weights.\n",
    "        if params is None:\n",
    "            self.params = {\n",
    "                'W': np.random.randn(n, self.num_classes),\n",
    "                'b': np.random.randn(1, self.num_classes)\n",
    "            }\n",
    "#             self.M = {\n",
    "#                 'W': np.zeros((n, self.num_classes)),\n",
    "#                 'b': np.zeros((1, self.num_classes))\n",
    "#             }\n",
    "#             self.V = {\n",
    "#                 'W': np.zeros((n, self.num_classes)),\n",
    "#                 'b': np.zeros((1, self.num_classes))\n",
    "#             }\n",
    "\n",
    "        cnt = 1\n",
    "        \n",
    "        # main loop: how much iterate on entire dataset.\n",
    "        for epoch in range(self.max_iter):\n",
    "            # before dive into SGD, shuffle dataset\n",
    "            X_shuffled, y_shuffled = self.shuffle(X, y_encoded)\n",
    "            \n",
    "            # cost variable for printing/logging\n",
    "            avg_loss = 0\n",
    "            \n",
    "            # batch_count = dataset_size / batch_size\n",
    "            batch_count = int(np.ceil(np.shape(X)[0] / self.batch_size))\n",
    "            \n",
    "            # mini-batch loop\n",
    "            for t in range(batch_count):\n",
    "                # draw the {batch_size} number of samples from X and y\n",
    "                X_batch, y_batch, bs = self.next_batch(X_shuffled, y_shuffled, t)\n",
    "                \n",
    "                # just in case, reshape batch of X and y into proper shape.\n",
    "                X_batch = np.reshape(X_batch, (bs, n))\n",
    "                y_batch = np.reshape(y_batch, (bs, self.num_classes))\n",
    "                \n",
    "                # prediction phase\n",
    "                Z = self.forward_prop(X_batch)\n",
    "                Z = np.reshape(Z, (bs, self.num_classes))\n",
    "                \n",
    "                # compute cost phase\n",
    "                loss = self.compute_cost(y_batch, Z)\n",
    "                \n",
    "                # update weights phase\n",
    "                self.backward_prop(X_batch, y_batch, Z, bs, cnt)\n",
    "                \n",
    "                # accumulate loss\n",
    "                avg_loss += loss\n",
    "                cnt += 1\n",
    "        \n",
    "            # logging\n",
    "            avg_loss /= batch_count\n",
    "            if epoch % (self.max_iter / 10) == 0:\n",
    "                print('Cost at epoch {0}: {1}'.format(epoch, avg_loss))\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def encode_y(self, y):\n",
    "        y_encoded = np.ones((np.shape(y)[0], self.num_classes))\n",
    "        \n",
    "        for i in range(self.num_classes):\n",
    "            y_encoded[:, i][y != i] = -1\n",
    "            \n",
    "        return y_encoded\n",
    "    \n",
    "    def shuffle(self, X, y):\n",
    "        \"\"\"\n",
    "        Random selection is required for SGD.\n",
    "        But, my approach is to shuffle entire data before every iteration.\n",
    "        This has same effect as random selection.\n",
    "        \n",
    "        Arguments:\n",
    "        ---------------------------\n",
    "        X: images (BATCH_SIZE, 784)\n",
    "        y: labels (BATCH_SIZE, 1)\n",
    "        \n",
    "        Returns:\n",
    "        ---------------------------\n",
    "        shuffled data\n",
    "        \"\"\"\n",
    "        \n",
    "        # the number of dataset samples\n",
    "        m = np.shape(X)[0]\n",
    "        \n",
    "        # variable for shuffle\n",
    "        r = np.arange(0, m)\n",
    "        \n",
    "        np.random.shuffle(r)\n",
    "        \n",
    "        return X[r], y[r]\n",
    "    \n",
    "    def next_batch(self, X, y, t):\n",
    "        \"\"\"\n",
    "        Get next batch.\n",
    "        If it is SGD, next_batch function just pick one sample from dataset.\n",
    "        \n",
    "        Arguments:\n",
    "        ---------------------------------\n",
    "        X: images (60000, 784)\n",
    "        y: labels (60000, 1)\n",
    "        \n",
    "        Returns:\n",
    "        ---------------------------------\n",
    "        X_batch: small subset of X (BATCH_SIZE, 784)\n",
    "        y_batch: small subset of y (BATCH_SIZE, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        # the number of dataset samples\n",
    "        m = np.shape(X)[0]\n",
    "        \n",
    "        # draw the {batch_size} number of samples from X and y\n",
    "        X_batch = X[t * self.batch_size : min(m, (t + 1) * self.batch_size)]\n",
    "        y_batch = y[t * self.batch_size : min(m, (t + 1) * self.batch_size)]\n",
    "        bs = min(m, (t + 1) * self.batch_size) - t * self.batch_size\n",
    "        \n",
    "        return X_batch, y_batch, bs\n",
    "    \n",
    "    def forward_prop(self, X):\n",
    "        \"\"\"\n",
    "        Process of inference (prediction).\n",
    "        \n",
    "        Arguments:\n",
    "        -----------------------\n",
    "        X: images e.g (BATCH_SIZE, 784)\n",
    "        params: weights dictionary(map in other programming language)\n",
    "        \n",
    "        Returns:\n",
    "        -----------------------\n",
    "        A: \n",
    "        \"\"\"\n",
    "        \n",
    "        # prediction\n",
    "        Z = np.matmul(X, self.params['W']) + self.params['b']\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "#     def sigmoid(self, Z):\n",
    "#         \"\"\"\n",
    "#         sigmoid activation for binary classification\n",
    "        \n",
    "#         Arguments:\n",
    "#         ----------------------\n",
    "#         Z: class score (W.T * X)\n",
    "        \n",
    "#         Returns:\n",
    "#         ----------------------\n",
    "#         sigmoid activation\n",
    "#         \"\"\"\n",
    "        \n",
    "#         return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    def compute_cost(self, y, Z):\n",
    "        \"\"\"\n",
    "        compute cost function (loss function)\n",
    "        \n",
    "        Arguments:\n",
    "        ------------------------------\n",
    "        y: true label\n",
    "        Z: class score (W.T * X)\n",
    "        \n",
    "        Returns:\n",
    "        ------------------------------\n",
    "        loss: total cost (loss)\n",
    "        \"\"\"\n",
    "        \n",
    "        # compute loss function\n",
    "        temp = 1 - np.multiply(y, Z)\n",
    "        temp[temp < 0] = 0\n",
    "        loss = np.mean(temp)\n",
    "        return loss\n",
    "    \n",
    "    def backward_prop(self, X, y, Z, bs, cnt):\n",
    "        \"\"\"\n",
    "        update weights\n",
    "        \n",
    "        Arguments:\n",
    "        ----------------------------\n",
    "        X: images e.g (BATCH_SIZE, 784)\n",
    "        y: labels e.g (BATCH_SIZE, 1)\n",
    "        Z: class score after forward propagation\n",
    "        params: weights dictionary(map in other programming language)\n",
    "        eta: learning rate\n",
    "        \n",
    "        Returns:\n",
    "        ----------------------------\n",
    "        params: weights dictionary\n",
    "        \"\"\"\n",
    "        \n",
    "        # number of features\n",
    "        n = np.shape(X)[1]\n",
    "        \n",
    "        # differential vector of loss function to update weights\n",
    "        dw = np.zeros(self.params['W'].shape)\n",
    "        db = np.zeros(self.params['b'].shape)\n",
    "        \n",
    "        Z = np.reshape(Z, (bs, self.num_classes))\n",
    "        temp = np.multiply(y, Z)\n",
    "        temp = 1 - temp\n",
    "        \n",
    "        temp[temp <= 0] = 0\n",
    "        temp[temp > 0] = 1\n",
    "        \n",
    "        y_temp = np.multiply(y, temp.reshape(bs, self.num_classes))\n",
    "        \n",
    "        dw = -(1 / bs) * np.matmul(X.T, y_temp) + (1 / self.C) * self.params['W']\n",
    "        db = -(1 / bs) * np.sum(y_temp, axis=0)\n",
    "\n",
    "#         if cnt == 1:\n",
    "#             self.M['W'] = dw\n",
    "#             self.M['b'] = db\n",
    "#         else:\n",
    "#             self.M['W'] = (self.beta1 * self.M['W'] + (1 - self.beta1) * dw)\n",
    "#             self.M['b'] = (self.beta1 * self.M['b'] + (1 - self.beta1) * db)\n",
    "        \n",
    "#         if cnt == 1:\n",
    "#             self.V['W'] = dw ** 2\n",
    "#             self.V['b'] = db ** 2\n",
    "#         else:\n",
    "#             self.V['W'] = (self.beta2 * self.V['W'] + (1 - self.beta2) * (dw ** 2))\n",
    "#             self.V['b'] = (self.beta2 * self.V['b'] + (1 - self.beta2) * (db ** 2))\n",
    "    \n",
    "#         self.M['W'] = (self.beta1 * self.M['W'] + (1 - self.beta1) * dw) / (1 - self.beta1 ** cnt)\n",
    "#         self.M['b'] = (self.beta1 * self.M['b'] + (1 - self.beta1) * db) / (1 - self.beta1 ** cnt)\n",
    "        \n",
    "#         self.V['W'] = (self.beta2 * self.V['W'] + (1 - self.beta2) * np.square(dw)) / (1 - self.beta2 ** cnt)\n",
    "#         self.V['b'] = (self.beta2 * self.V['b'] + (1 - self.beta2) * np.square(db)) / (1 - self.beta2 ** cnt)\n",
    "\n",
    "        # update weights\n",
    "#         self.params['W'] = self.params['W'] - np.divide(self.eta * self.M['W'], np.sqrt(self.V['W']) + self.epsilon)\n",
    "#         self.params['b'] = self.params['b'] - np.divide(self.eta * self.M['b'], np.sqrt(self.V['b']) + self.epsilon)\n",
    "        \n",
    "        self.params['W'] = self.params['W'] - (self.eta / (1 + self.epsilon * cnt)) * dw\n",
    "        self.params['b'] = self.params['b'] - (self.eta / (1 + self.epsilon * cnt)) * db\n",
    "\n",
    "#         self.params['W'] = self.params['W'] - (self.eta / (1 + self.epsilon * cnt)) * dw\n",
    "#         self.params['b'] = self.params['b'] - (self.eta / (1 + self.epsilon * cnt)) * db\n",
    "        \n",
    "        return self.params\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        m = np.shape(X)[0]\n",
    "        \n",
    "        class_score = self.forward_prop(X)\n",
    "        pred = np.argmax(class_score, axis=1)\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    def score(self, X, y=None):\n",
    "        pred = self.predict(X)\n",
    "        score = np.mean(pred == y)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        return self.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_padding(images, n=1):\n",
    "    \"\"\"\n",
    "    zero-padding to image.\n",
    "    add additional edge which has value of 0\n",
    "    \n",
    "    Arguments:\n",
    "    ---------------------\n",
    "    - images: training dataset images. maybe (60000, 28, 28)\n",
    "    - n: how many padding do you want? in other word, how many edge do you want to insert?\n",
    "    \n",
    "    Returns:\n",
    "    ---------------------\n",
    "    - images_padded: padded images. (60000, 30, 30) or other shape.\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of training examples. 60000. if you use test data, 10000.\n",
    "    m = images.shape[0]\n",
    "    \n",
    "    # define larger size of window than size of images. maybe (60000, 30, 30), (60000, 32, 32)\n",
    "    images_padded = np.zeros((m, images.shape[1] + 2 * n, images.shape[2] + 2 * n))\n",
    "    \n",
    "    # insert image in the middle of this window.\n",
    "    images_padded[:, n : images_padded.shape[1] - n, n : images_padded.shape[2] - n] = images\n",
    "    \n",
    "    return images_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_gradient_slice(images_slice):\n",
    "    \"\"\"\n",
    "    find gradient(in korean, 기울기 또는 미분값) for part of images.\n",
    "    \n",
    "    Arguments:\n",
    "    ----------------------\n",
    "    - images_slice: small window extracted from images. (60000, 7, 7)\n",
    "    \n",
    "    Returns:\n",
    "    ----------------------\n",
    "    - grad: x-axis-oriented gradient (in korean, x 축 방향 기울기)\n",
    "    \"\"\"\n",
    "    \n",
    "    x_gradient_filter = np.array([\n",
    "        [-1,  0,  1],\n",
    "        [-1,  0,  1],\n",
    "        [-1,  0,  1],\n",
    "    ])\n",
    "    \n",
    "    # reshape for broadcasting.\n",
    "    x_gradient_filter = x_gradient_filter.reshape(1, 3, 3)\n",
    "    \n",
    "    # element-wise compute. compute gradient\n",
    "    temp = np.multiply(images_slice, x_gradient_filter)\n",
    "    grad = np.sum(temp, axis=(1, 2))\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_gradient(images):\n",
    "    \"\"\"\n",
    "    find gradient(in korean, 기울기 또는 미분값) for whole images.\n",
    "    \n",
    "    Arguments:\n",
    "    ----------------------\n",
    "    - images: images. (60000, 28, 28)\n",
    "    \n",
    "    Returns:\n",
    "    ----------------------\n",
    "    - grad: x-axis-oriented gradient (in korean, x 축 방향 기울기)\n",
    "    \"\"\"\n",
    "    \n",
    "    # some useful variables.\n",
    "    m = images.shape[0]\n",
    "    width = images.shape[1]\n",
    "    height = images.shape[2]\n",
    "    \n",
    "    # define placeholder to store gradients.\n",
    "    x_grads = np.zeros((m, width - 2, height - 2))\n",
    "    \n",
    "    # slice image into small size window, then compute gradient.\n",
    "    for w in range(1, width - 1):\n",
    "        for h in range(1, height - 1):\n",
    "            images_slice = images[:, w - 1 : w + 2, h - 1 : h + 2]\n",
    "            x_grads[:, w - 1, h - 1] = x_gradient_slice(images_slice)\n",
    "            \n",
    "    return x_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_gradient_slice(images_slice):\n",
    "    \"\"\"\n",
    "    find gradient(in korean, 기울기 또는 미분값) for part of images.\n",
    "    \n",
    "    Arguments:\n",
    "    ----------------------\n",
    "    - images_slice: small window extracted from images. (60000, 7, 7)\n",
    "    \n",
    "    Returns:\n",
    "    ----------------------\n",
    "    - grad: y-axis-oriented gradient (in korean, y 축 방향 기울기)\n",
    "    \"\"\"\n",
    "    \n",
    "    y_gradient_filter = np.array([\n",
    "        [-1, -1, -1],\n",
    "        [ 0,  0,  0],\n",
    "        [ 1,  1,  1],\n",
    "    ])\n",
    "    \n",
    "    # reshape for broadcasting.\n",
    "    y_gradient_filter = y_gradient_filter.reshape(1, 3, 3)\n",
    "    \n",
    "    # element-wise compute. compute gradient\n",
    "    temp = np.multiply(images_slice, y_gradient_filter)\n",
    "    grad = np.sum(temp, axis=(1, 2))\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_gradient(images):\n",
    "    \"\"\"\n",
    "    find gradient(in korean, 기울기 또는 미분값) for whole images.\n",
    "    \n",
    "    Arguments:\n",
    "    ----------------------\n",
    "    - images: images. (60000, 28, 28)\n",
    "    \n",
    "    Returns:\n",
    "    ----------------------\n",
    "    - grad: y-axis-oriented gradient (in korean, y 축 방향 기울기)\n",
    "    \"\"\"\n",
    "    \n",
    "    # some useful variables.\n",
    "    m = images.shape[0]\n",
    "    width = images.shape[1]\n",
    "    height = images.shape[2]\n",
    "    \n",
    "    # define placeholder to store gradients.\n",
    "    y_grads = np.zeros((m, width - 2, height - 2))\n",
    "    \n",
    "    # slice image into small size window, then compute gradient.\n",
    "    for w in range(1, width - 1):\n",
    "        for h in range(1, height - 1):\n",
    "            images_slice = images[:, w - 1 : w + 2, h - 1 : h + 2]\n",
    "            y_grads[:, w - 1, h - 1] = y_gradient_slice(images_slice)\n",
    "            \n",
    "    return y_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_grads(x_grads, y_grads, grid=7):\n",
    "    \"\"\"\n",
    "    After we get gradients, let's compute average of these gradient. I'will post pictures.\n",
    "    compute partial gradients.\n",
    "    \n",
    "    Arguments:\n",
    "    ---------------------\n",
    "    - x_grads: pre-computed gradients for x-axis (60000, 28, 28)\n",
    "    - y_grads: pre-computed gradients for y-axis (60000, 28, 28)\n",
    "    - grid: grid for dividing images. we will compute average of gradients for each grid. the averages become features.\n",
    "    \n",
    "    Returns:\n",
    "    ---------------------\n",
    "    - x_avg_grads: average of gradients x-axis (60000, 7, 7)\n",
    "    - y_avg_grads: average of gradients y-axis (60000, 7, 7)\n",
    "    \"\"\"\n",
    "    \n",
    "    assert(x_grads.shape == y_grads.shape)\n",
    "    \n",
    "    # some useful variables.\n",
    "    m = x_grads.shape[0]\n",
    "    width = x_grads.shape[1]\n",
    "    height = x_grads.shape[2]\n",
    "    \n",
    "    # I define these variables to slicing images conveniently.\n",
    "    w_step = width // grid  # w_step = 4\n",
    "    h_step = height // grid # h_step = 4\n",
    "    \n",
    "    # placeholder for storing average of gradients\n",
    "    x_avg_grads = np.zeros((m, width // w_step, height // h_step))\n",
    "    y_avg_grads = np.zeros((m, width // w_step, height // h_step))\n",
    "    \n",
    "    for w in range(0, width, w_step):\n",
    "        for h in range(0, height, h_step):\n",
    "            # slicing gradients into small part.\n",
    "            x_grads_slice = x_grads[:, w : w + w_step, h : h + h_step]\n",
    "            y_grads_slice = y_grads[:, w : w + w_step, h : h + h_step]\n",
    "            \n",
    "            assert(x_grads_slice.shape == y_grads_slice.shape == (m, width // grid, height // grid))\n",
    "            \n",
    "            # compute mean of gradients of part of image\n",
    "            x_avg_grads[:, w // w_step, h // h_step] = np.mean(x_grads_slice, axis=(1, 2))\n",
    "            y_avg_grads[:, w // w_step, h // h_step] = np.mean(y_grads_slice, axis=(1, 2))\n",
    "            \n",
    "    return x_avg_grads, y_avg_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_gradients_grid(images, grid=7, padding=1, normalize=True):\n",
    "    \"\"\"\n",
    "    Preprocessing method 1 which I tried.\n",
    "    \n",
    "    Arguments:\n",
    "    -------------------------\n",
    "    - images: training or test images (60000, 28, 28)\n",
    "    - grid: grid for dividing images. we will compute average of gradients for each grid. the averages become features.\n",
    "    - padding: how much padding image.\n",
    "    \n",
    "    Returns:\n",
    "    -------------------------\n",
    "    - features: pre-processed features (pixel of images). (60000, 98)\n",
    "    \"\"\"\n",
    "    \n",
    "    images = np.copy(images)\n",
    "    \n",
    "    m = images.shape[0]\n",
    "    \n",
    "    # normalize\n",
    "    if normalize:\n",
    "        images_norm = images / 255\n",
    "    else:\n",
    "        images_norm = images\n",
    "    \n",
    "    # thresholding\n",
    "    images_norm[images_norm >= 0.1] = 1\n",
    "    images_norm[images_norm < 0.1] = 0\n",
    "    \n",
    "    # zero padding\n",
    "    images_padded = zero_padding(images_norm, padding)\n",
    "    \n",
    "    # number of features = grid^2 * 2\n",
    "    features = np.zeros((m, (grid ** 2) * 2))\n",
    "\n",
    "    # compute x-axis gradient, y-axis gradient\n",
    "    x_grads = x_gradient(images_padded)\n",
    "    y_grads = y_gradient(images_padded)\n",
    "    \n",
    "    # compute average of gradient (grid 7x7)\n",
    "    x_avg_grads, y_avg_grads = get_average_grads(x_grads, y_grads, grid)\n",
    "    \n",
    "    assert(x_avg_grads.shape == y_avg_grads.shape == (m, grid, grid))\n",
    "    \n",
    "    # flatten\n",
    "    x_features = x_avg_grads.reshape(m, -1)\n",
    "    y_features = y_avg_grads.reshape(m, -1)\n",
    "    \n",
    "    features[:, 0 :: 2] = x_features\n",
    "    features[:, 1 :: 2] = y_features\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_hog(images_train, images_test):\n",
    "    m_tr = images_train.shape[0]\n",
    "    m_ts = images_test.shape[0]\n",
    "    \n",
    "    # compute HoG (Historgram of Gradients)\n",
    "    hog_train = np.zeros((m_tr, 81))\n",
    "    hog_test = np.zeros((m_ts, 81))\n",
    "    \n",
    "    for i in range(m_tr):\n",
    "        hog_train[i] = hog(images_train[i], block_norm='L2-Hys')\n",
    "    for i in range(m_ts):\n",
    "        hog_test[i] = hog(images_test[i], block_norm='L2-Hys')\n",
    "        \n",
    "    return hog_train, hog_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_model(grads, hogs, poly_degree):\n",
    "    \"\"\"\n",
    "    This function reads inputs (grads, hogs) and then append into one vector.\n",
    "    And, most importantly, make the features (grads + hogs vector) polynomial or exponential\n",
    "    This has an effect that makes algorithm be applied to non-linear-separatable dataset.\n",
    "    \n",
    "    Arguments\n",
    "    ---------------------------------\n",
    "    grads: features containing gradients of images\n",
    "    hogs: features containing histogram of gradients\n",
    "    \n",
    "    Returns\n",
    "    ---------------------------------\n",
    "    f_train: preprocessed features of training dataset this function generate\n",
    "    f_test: preprocessed features of test dataset this function generate\n",
    "    \"\"\"\n",
    "    \n",
    "    # get number of training set (60000), number of test set (10000)\n",
    "    m_train = grads[0].shape[0] # 60000\n",
    "    m_test = grads[1].shape[0]  # 10000\n",
    "    \n",
    "#     # placeholder for new features.\n",
    "#     f_train = np.zeros((m_train, (98 + 81) * poly_degree))\n",
    "#     f_test = np.zeros((m_test, (98 + 81) * poly_degree)) \n",
    "    \n",
    "    grads_train, grads_test = grads[0], grads[1]\n",
    "    hogs_train, hogs_test = hogs[0], hogs[1]\n",
    "    \n",
    "#     for i in range(poly_degree):\n",
    "#         f_train[:, (98 + 81) * i : (98 + 81) * i + 98] = grads_train ** (i + 1)\n",
    "#         f_train[:, (98 + 81) * i + 98 : (98 + 81) * (i + 1)] = hogs_train ** (i + 1)\n",
    "#         f_test[:, (98 + 81) * i : (98 + 81) * i + 98] = grads_test ** (i + 1)\n",
    "#         f_test[:, (98 + 81) * i + 98 : (98 + 81) * (i + 1)] = hogs_test ** (i + 1)\n",
    "        \n",
    "    # 97.0\n",
    "    f_train = np.zeros((m_train, (98 + 81) * poly_degree + 49 + 168 * 2))\n",
    "    f_test = np.zeros((m_test, (98 + 81) * poly_degree + 49 + 168 * 2))\n",
    "    \n",
    "    \n",
    "#     f_train = np.zeros((m_train, 98 * poly_degree + 168 * 2))\n",
    "#     f_test = np.zeros((m_test, 98 * poly_degree + 168 * 2))\n",
    "    \n",
    "    for i in range(poly_degree):\n",
    "        f_train[:, 98 * i : 98 * (i + 1)] = grads_train ** (i + 1)\n",
    "        f_test[:, 98 * i : 98 * (i + 1)] = grads_test ** (i + 1)\n",
    "    \n",
    "    cur_w = 0\n",
    "    cur_h = 0\n",
    "    cur_idx = 98 * poly_degree\n",
    "    \n",
    "    while True:\n",
    "        if cur_w == 7:\n",
    "            cur_w = 0\n",
    "            cur_h += 1\n",
    "        if cur_h == 7:\n",
    "            break\n",
    "            \n",
    "        # print(cur_h, cur_w)        \n",
    "        \n",
    "        if cur_w < 6:\n",
    "            f_train[:, cur_idx] = np.multiply(grads_train[:, cur_h * 14 + cur_w * 2], grads_train[:, cur_h * 14 + cur_w * 2 + 2])\n",
    "            f_test[:, cur_idx] = np.multiply(grads_test[:, cur_h * 14 + cur_w * 2], grads_test[:, cur_h * 14 + cur_w * 2 + 2])\n",
    "            cur_idx += 1\n",
    "            \n",
    "            f_train[:, cur_idx] = np.multiply(grads_train[:, cur_h * 14 + cur_w * 2 + 1], grads_train[:, cur_h * 14 + cur_w * 2 + 3])\n",
    "            f_test[:, cur_idx] = np.multiply(grads_test[:, cur_h * 14 + cur_w * 2 + 1], grads_test[:, cur_h * 14 + cur_w * 2 + 3])\n",
    "            cur_idx += 1\n",
    "            \n",
    "        if cur_h < 6:\n",
    "            f_train[:, cur_idx] = np.multiply(grads_train[:, cur_h * 14 + cur_w * 2], grads_train[:, cur_h * 14 + cur_w * 2 + 14])\n",
    "            f_test[:, cur_idx] = np.multiply(grads_test[:, cur_h * 14 + cur_w * 2], grads_test[:, cur_h * 14 + cur_w * 2 + 14])\n",
    "            cur_idx += 1\n",
    "            \n",
    "            f_train[:, cur_idx] = np.multiply(grads_train[:, cur_h * 14 + cur_w * 2 + 1], grads_train[:, cur_h * 14 + cur_w * 2 + 15])\n",
    "            f_test[:, cur_idx] = np.multiply(grads_test[:, cur_h * 14 + cur_w * 2 + 1], grads_test[:, cur_h * 14 + cur_w * 2 + 15])\n",
    "            cur_idx += 1\n",
    "            \n",
    "        cur_w += 1\n",
    "            \n",
    "    assert(cur_idx == 98 * poly_degree + 168)\n",
    "    \n",
    "    \n",
    "    cur_w = 0\n",
    "    cur_h = 0\n",
    "    cur_idx = 98 * poly_degree + 168\n",
    "    \n",
    "    while True:\n",
    "        if cur_w == 7:\n",
    "            cur_w = 0\n",
    "            cur_h += 1\n",
    "        if cur_h == 7:\n",
    "            break\n",
    "            \n",
    "        # print(cur_h, cur_w)        \n",
    "        \n",
    "        if cur_w < 6:\n",
    "            f_train[:, cur_idx] = np.multiply(grads_train[:, cur_h * 14 + cur_w * 2]**2, grads_train[:, cur_h * 14 + cur_w * 2 + 2]**2)\n",
    "            f_test[:, cur_idx] = np.multiply(grads_test[:, cur_h * 14 + cur_w * 2]**2, grads_test[:, cur_h * 14 + cur_w * 2 + 2]**2)\n",
    "            cur_idx += 1\n",
    "            \n",
    "            f_train[:, cur_idx] = np.multiply(grads_train[:, cur_h * 14 + cur_w * 2 + 1]**2, grads_train[:, cur_h * 14 + cur_w * 2 + 3]**2)\n",
    "            f_test[:, cur_idx] = np.multiply(grads_test[:, cur_h * 14 + cur_w * 2 + 1]**2, grads_test[:, cur_h * 14 + cur_w * 2 + 3]**2)\n",
    "            cur_idx += 1\n",
    "            \n",
    "        if cur_h < 6:\n",
    "            f_train[:, cur_idx] = np.multiply(grads_train[:, cur_h * 14 + cur_w * 2]**2, grads_train[:, cur_h * 14 + cur_w * 2 + 14]**2)\n",
    "            f_test[:, cur_idx] = np.multiply(grads_test[:, cur_h * 14 + cur_w * 2]**2, grads_test[:, cur_h * 14 + cur_w * 2 + 14]**2)\n",
    "            cur_idx += 1\n",
    "            \n",
    "            f_train[:, cur_idx] = np.multiply(grads_train[:, cur_h * 14 + cur_w * 2 + 1]**2, grads_train[:, cur_h * 14 + cur_w * 2 + 15]**2)\n",
    "            f_test[:, cur_idx] = np.multiply(grads_test[:, cur_h * 14 + cur_w * 2 + 1]**2, grads_test[:, cur_h * 14 + cur_w * 2 + 15]**2)\n",
    "            cur_idx += 1\n",
    "            \n",
    "        cur_w += 1\n",
    "        \n",
    "    assert(cur_idx == 98 * poly_degree + 168*2)\n",
    "    \n",
    "    f_train[:, cur_idx : cur_idx + 49] = np.multiply(grads_train[:, 0 :: 2], grads_train[:, 1 :: 2])\n",
    "    f_test[:, cur_idx : cur_idx + 49] = np.multiply(grads_test[:, 0 :: 2], grads_test[:, 1 :: 2])\n",
    "    \n",
    "    cur_idx += 49\n",
    "    \n",
    "    for i in range(poly_degree):\n",
    "        f_train[:, cur_idx + 81 * i : cur_idx + 81 * (i + 1)] = hogs_train  ** (i + 1)\n",
    "        f_test[:, cur_idx + 81 * i : cur_idx + 81 * (i + 1)] = hogs_test ** (i + 1)\n",
    "        \n",
    "    cur_idx += 81 * poly_degree\n",
    "    \n",
    "    ### 97.0\n",
    "        \n",
    "    return f_train, f_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_poly_features(images_train, images_test, poly_degree=3):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    -------------------------\n",
    "    images_train: images in training datase shaped of (60000, 28, 28)\n",
    "    images_test: images in test dataset shaped of (10000, 28, 28)\n",
    "    poly_degree: how many do you product exponentialy?\n",
    "    \n",
    "    Returns\n",
    "    -------------------------\n",
    "    f_train: new features preprocessed\n",
    "    f_test: new features of test images.\n",
    "    \"\"\"\n",
    "    \n",
    "    m_train = images_train.shape[0]\n",
    "    m_test = images_test.shape[0]\n",
    "    \n",
    "    # compute average of gradients\n",
    "    avg_grads_train = average_gradients_grid(images_train)\n",
    "    avg_grads_test = average_gradients_grid(images_test)\n",
    "    \n",
    "    hog_train, hog_test = apply_hog(images_train, images_test)\n",
    "        \n",
    "    f_train, f_test = poly_model((avg_grads_train, avg_grads_test), (hog_train, hog_test), poly_degree)\n",
    "    \n",
    "    return f_train, f_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 922)\n",
      "(10000, 922)\n"
     ]
    }
   ],
   "source": [
    "X_train1, X_test1 = make_poly_features(images_train, images_test)\n",
    "\n",
    "print(X_train1.shape)\n",
    "print(X_test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 200)\n",
      "(10000, 200)\n"
     ]
    }
   ],
   "source": [
    "X_train1_1, X_test1_1 = principal_component_analysis(X_train1, X_test1, n_components=200)\n",
    "\n",
    "print(X_train1_1.shape)\n",
    "print(X_test1_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 1280)\n",
      "(10000, 1280)\n"
     ]
    }
   ],
   "source": [
    "X_train1_1, X_test1_1 = principal_component_analysis\n",
    "\n",
    "print(X_train1.shape)\n",
    "print(X_test1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Presentation (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== class 0 =====\n",
      "Cost at epoch 0: 10.686779661599083\n",
      "Cost at epoch 20: 3.9106110524101063\n",
      "Cost at epoch 40: 3.8943336857758077\n",
      "Cost at epoch 60: 3.9469301067847753\n",
      "Cost at epoch 80: 3.8709681574980714\n",
      "===== class 1 =====\n",
      "Cost at epoch 0: 7.776986236755182\n",
      "Cost at epoch 20: 2.4891427342652643\n",
      "Cost at epoch 40: 2.3798333476428706\n",
      "Cost at epoch 60: 2.4464050664817494\n",
      "Cost at epoch 80: 2.4073779035658793\n",
      "===== class 2 =====\n",
      "Cost at epoch 0: 15.264107177343247\n",
      "Cost at epoch 20: 8.449161511196248\n",
      "Cost at epoch 40: 8.63409637115864\n",
      "Cost at epoch 60: 8.622411739425603\n",
      "Cost at epoch 80: 8.464533323611493\n",
      "===== class 3 =====\n",
      "Cost at epoch 0: 18.280004966446374\n",
      "Cost at epoch 20: 11.052509601646479\n",
      "Cost at epoch 40: 10.850239516875225\n",
      "Cost at epoch 60: 10.860347798842836\n",
      "Cost at epoch 80: 10.90418745211039\n",
      "===== class 4 =====\n",
      "Cost at epoch 0: 12.627727763634088\n",
      "Cost at epoch 20: 5.869691972435452\n",
      "Cost at epoch 40: 5.920136742306459\n",
      "Cost at epoch 60: 5.779820081917636\n",
      "Cost at epoch 80: 5.796789300967916\n",
      "===== class 5 =====\n",
      "Cost at epoch 0: 19.149310850858246\n",
      "Cost at epoch 20: 11.13590820160952\n",
      "Cost at epoch 40: 11.320484050686836\n",
      "Cost at epoch 60: 11.422617568307922\n",
      "Cost at epoch 80: 11.347355817397734\n",
      "===== class 6 =====\n",
      "Cost at epoch 0: 12.259911271575481\n",
      "Cost at epoch 20: 5.275648091586775\n",
      "Cost at epoch 40: 5.432739489666828\n",
      "Cost at epoch 60: 5.370637220237479\n",
      "Cost at epoch 80: 5.377073634527475\n",
      "===== class 7 =====\n",
      "Cost at epoch 0: 11.215394514965984\n",
      "Cost at epoch 20: 5.3961048831078005\n",
      "Cost at epoch 40: 5.328104863184484\n",
      "Cost at epoch 60: 5.297998422253531\n",
      "Cost at epoch 80: 5.334388557177119\n",
      "===== class 8 =====\n",
      "Cost at epoch 0: 28.344835711385674\n",
      "Cost at epoch 20: 20.579964836982775\n",
      "Cost at epoch 40: 20.4232439162097\n",
      "Cost at epoch 60: 20.4450001503242\n",
      "Cost at epoch 80: 20.446095564464933\n",
      "===== class 9 =====\n",
      "Cost at epoch 0: 20.010391524626893\n",
      "Cost at epoch 20: 13.89520065736062\n",
      "Cost at epoch 40: 14.1079801762105\n",
      "Cost at epoch 60: 13.813032913766529\n",
      "Cost at epoch 80: 13.943312493163662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyMultiClassSVM(C=0.1, batch_size=1, max_iter=100)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm1 = MyMultiClassSVM(C=0.1, batch_size=1, max_iter=100)\n",
    "svm1.fit(images_train.reshape(-1, 784), labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82805"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm1.score(images_train.reshape(-1, 784), labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.827"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm1.score(images_test.reshape(-1, 784), labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Second Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0: 498.5804958412841\n",
      "Cost at epoch 1: 359.10277278900776\n",
      "Cost at epoch 2: 355.71068985186423\n",
      "Cost at epoch 3: 359.609476224103\n",
      "Cost at epoch 4: 356.2373117152157\n",
      "Cost at epoch 5: 345.3893770677822\n",
      "Cost at epoch 6: 355.2519745700933\n",
      "Cost at epoch 7: 353.24436355337383\n",
      "Cost at epoch 8: 356.68547173822924\n",
      "Cost at epoch 9: 355.21175859380486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MySVM(C=100, batch_size=128, epsilon=1e-06, eta=0.1, max_iter=10)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm9 = MySVM(C=100, max_iter=10, batch_size=128, eta=0.1, epsilon=1e-6)\n",
    "svm9.fit(images_train.reshape(-1, 784), labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82295\n",
      "0.8261\n"
     ]
    }
   ],
   "source": [
    "print(svm9.score(images_train.reshape(-1, 784), labels_train))\n",
    "print(svm9.score(images_test.reshape(-1, 784), labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0: 0.5035333226658423\n",
      "Cost at epoch 1: 0.10438156594947337\n",
      "Cost at epoch 2: 0.06016534104111028\n",
      "Cost at epoch 3: 0.04570331757865704\n",
      "Cost at epoch 4: 0.04146340068239925\n",
      "Cost at epoch 5: 0.03995063634935078\n",
      "Cost at epoch 6: 0.03954779149909941\n",
      "Cost at epoch 7: 0.03932526051044747\n",
      "Cost at epoch 8: 0.039288615859364814\n",
      "Cost at epoch 9: 0.03920007023660946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MySVM(C=100, batch_size=128, epsilon=1e-06, eta=0.1, max_iter=10)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm10 = MySVM(C=100, max_iter=10, batch_size=128, eta=0.1, epsilon=1e-6)\n",
    "svm10.fit(X_train1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94905\n",
      "0.9541\n"
     ]
    }
   ],
   "source": [
    "print(svm10.score(X_train1, y_train))\n",
    "print(svm10.score(X_test1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0: 0.4868896166333449\n",
      "Cost at epoch 14: 0.034642926555208654\n",
      "Cost at epoch 28: 0.024553674948518237\n",
      "Cost at epoch 42: 0.023249654502907918\n",
      "Cost at epoch 56: 0.023045071307159416\n",
      "Cost at epoch 70: 0.02292480221704541\n",
      "Cost at epoch 84: 0.022888446619541123\n",
      "Cost at epoch 98: 0.022854011039341877\n",
      "Cost at epoch 112: 0.022835906738977757\n",
      "Cost at epoch 126: 0.022808043309768867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MySVM(C=1000, batch_size=128, epsilon=1e-08, eta=0.1, max_iter=140)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm11 = MySVM(C=1000, max_iter=140, batch_size=128, eta=0.1)\n",
    "svm11.fit(X_train1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9707833333333333\n",
      "0.9692\n"
     ]
    }
   ],
   "source": [
    "print(svm11.score(X_train1, y_train))\n",
    "print(svm11.score(X_test1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0: 0.7334484125818558\n",
      "Cost at epoch 15: 0.05274669657420555\n",
      "Cost at epoch 30: 0.03216483529590398\n",
      "Cost at epoch 45: 0.025873836753252496\n",
      "Cost at epoch 60: 0.02370003559416378\n",
      "Cost at epoch 75: 0.022996210569053436\n",
      "Cost at epoch 90: 0.02262936586380937\n",
      "Cost at epoch 105: 0.0225053841804381\n",
      "Cost at epoch 120: 0.022413567913048153\n",
      "Cost at epoch 135: 0.02226538391143299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MySVM(C=1000, batch_size=256, epsilon=1e-06, eta=0.1, max_iter=150)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm12 = MySVM(C=1000, max_iter=150, batch_size=256, eta=0.1, epsilon=1e-6)\n",
    "svm12.fit(X_train1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9715\n",
      "0.9704\n"
     ]
    }
   ],
   "source": [
    "print(svm12.score(X_train1, y_train))\n",
    "print(svm12.score(X_test1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0: 0.23133134841341058\n",
      "Cost at epoch 6: 0.03466896102249104\n",
      "Cost at epoch 12: 0.025818586243018006\n",
      "Cost at epoch 18: 0.022909658473664194\n",
      "Cost at epoch 24: 0.02202936745164572\n",
      "Cost at epoch 30: 0.021419137597926453\n",
      "Cost at epoch 36: 0.021065128861031145\n",
      "Cost at epoch 42: 0.021047066704588894\n",
      "Cost at epoch 48: 0.020946851930114262\n",
      "Cost at epoch 54: 0.020999108051241756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MySVM(C=5000, batch_size=128, eta=0.5, max_iter=60)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm12 = MySVM(C=5000, max_iter=60, batch_size=128, eta=0.5)\n",
    "svm12.fit(X_train1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97465\n",
      "0.9721\n"
     ]
    }
   ],
   "source": [
    "print(svm12.score(X_train1, y_train))\n",
    "print(svm12.score(X_test1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0: 0.2183647436927403\n",
      "Cost at epoch 6: 0.03171837887838108\n",
      "Cost at epoch 12: 0.02544136402485873\n",
      "Cost at epoch 18: 0.02415090152977139\n",
      "Cost at epoch 24: 0.0236846273495908\n",
      "Cost at epoch 30: 0.02317945789227962\n",
      "Cost at epoch 36: 0.02319328806980065\n",
      "Cost at epoch 42: 0.023551674518366005\n",
      "Cost at epoch 48: 0.023035735109379247\n",
      "Cost at epoch 54: 0.023318888672678104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MySVM(C=3000, batch_size=128, eta=0.5, max_iter=60)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm13 = MySVM(C=3000, max_iter=60, batch_size=128, eta=0.5)\n",
    "svm13.fit(X_train1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9673166666666667\n",
      "0.9645\n"
     ]
    }
   ],
   "source": [
    "print(svm13.score(X_train1, y_train))\n",
    "print(svm13.score(X_test1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0: 0.5178958171835167\n",
      "Cost at epoch 16: 0.03581916261548705\n",
      "Cost at epoch 32: 0.024306085129678332\n",
      "Cost at epoch 48: 0.021984525199011534\n",
      "Cost at epoch 64: 0.021376543774205965\n",
      "Cost at epoch 80: 0.0211023774133448\n",
      "Cost at epoch 96: 0.020948865127737144\n",
      "Cost at epoch 112: 0.020879688687241153\n",
      "Cost at epoch 128: 0.0208613647427255\n",
      "Cost at epoch 144: 0.020786574306057504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MySVM(C=1500, batch_size=128, epsilon=1e-06, eta=0.1, max_iter=160)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm14 = MySVM(C=1500, max_iter=160, batch_size=128, eta=0.1, epsilon=1e-6)\n",
    "svm14.fit(X_train1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9737833333333333\n",
      "0.9706\n"
     ]
    }
   ],
   "source": [
    "print(svm14.score(X_train1, y_train))\n",
    "print(svm14.score(X_test1, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Grid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0: 0.5062394022425962\n",
      "Cost at epoch 10: 0.055337102693834635\n",
      "Cost at epoch 20: 0.03557037364159835\n",
      "Cost at epoch 30: 0.028561444389302318\n",
      "Cost at epoch 40: 0.02545134005411057\n",
      "Cost at epoch 50: 0.024058577126849626\n",
      "Cost at epoch 60: 0.023363414666039913\n",
      "Cost at epoch 70: 0.02307407703963969\n",
      "Cost at epoch 80: 0.022848414855454954\n",
      "Cost at epoch 90: 0.02281231330480708\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C': [100, 300, 1000, 3000],\n",
    "    'batch_size': [128, 256, 512],\n",
    "    'max_iter': [60, 100, 140],\n",
    "    'epsilon': [1e-8, 1e-6],\n",
    "    'eta': [0.1]\n",
    "}\n",
    "\n",
    "svm15 = get_model(MySVM, 'data/MySVM3.data', param_grid=param_grid, X=X_train1_1, Y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 3000, 'batch_size': 128, 'epsilon': 1e-06, 'eta': 0.1, 'max_iter': 100}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm15.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96955\n",
      "0.9678\n"
     ]
    }
   ],
   "source": [
    "print(svm15.score(X_train1_1, y_train))\n",
    "print(svm15.score(X_test1_1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0: 0.4124635042119067\n",
      "Cost at epoch 14: 0.05287199149041953\n",
      "Cost at epoch 28: 0.043367706013074596\n",
      "Cost at epoch 42: 0.04060016037574976\n",
      "Cost at epoch 56: 0.03951733258821541\n",
      "Cost at epoch 70: 0.03897316008802417\n",
      "Cost at epoch 84: 0.038694802772646156\n",
      "Cost at epoch 98: 0.038487977965632435\n",
      "Cost at epoch 112: 0.03840115744979289\n",
      "Cost at epoch 126: 0.038281154735352485\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C': [2500, 3000, 3500, 4000],\n",
    "    'batch_size': [128, 256],\n",
    "    'max_iter': [60, 100, 140],\n",
    "    'epsilon': [1e-8, 1e-6],\n",
    "    'eta': [0.1]\n",
    "}\n",
    "\n",
    "svm16 = get_model(MySVM, 'data/MySVM4.data', param_grid=param_grid, X=X_train1_2, Y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9412333333333334"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm16.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [100, 300, 1000, 3000],\n",
    "    'batch_size': [128, 256, 512],\n",
    "    'max_iter': [60, 100, 140],\n",
    "    'epsilon': [1e-8, 1e-6],\n",
    "    'eta': [0.1]\n",
    "}\n",
    "\n",
    "svm29 = get_model(MySVM, 'data/MySVM1.data', param_grid=param_grid, X=X_train1, Y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9704833333333334\n",
      "{'C': 3000, 'batch_size': 128, 'epsilon': 1e-06, 'eta': 0.1, 'max_iter': 140}\n",
      "{'W': array([[ 0.13710753,  0.11072044,  0.30610425, ..., -0.01590253,\n",
      "        -0.19389575,  0.06185292],\n",
      "       [ 0.19278389, -0.04250993,  0.13152295, ...,  0.10308245,\n",
      "        -0.08123604,  0.05646114],\n",
      "       [ 0.14558454, -0.13749245, -0.05957599, ..., -0.07933711,\n",
      "         0.12258241, -0.08383579],\n",
      "       ...,\n",
      "       [ 0.09188823,  0.13741366,  0.27188086, ...,  0.08381936,\n",
      "        -0.15180401, -0.00396859],\n",
      "       [-0.19813818, -0.13988205,  0.0035366 , ..., -0.19149217,\n",
      "        -0.00095808, -0.1070209 ],\n",
      "       [ 0.08765961,  0.11305135,  0.21979072, ..., -0.02604575,\n",
      "        -0.06398772, -0.10612421]]), 'b': array([[-1.66203954,  0.10457154, -0.98303182, -1.73397415, -2.6234894 ,\n",
      "        -0.1175283 , -1.73561003, -1.41792256, -3.94602472, -1.82123815]])}\n",
      "0.9766\n",
      "0.9721\n"
     ]
    }
   ],
   "source": [
    "print(svm29.best_score_)\n",
    "print(svm29.best_params_)\n",
    "print(svm29.best_estimator_.get_parameters())\n",
    "\n",
    "print(svm29.best_estimator_.score(X_train1, y_train))\n",
    "print(svm29.best_estimator_.score(X_test1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split5_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(svm29.cv_results_).to_csv('MySVM_svm29_train1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W': array([[ 0.13710753,  0.11072044,  0.30610425, ..., -0.01590253,\n",
      "        -0.19389575,  0.06185292],\n",
      "       [ 0.19278389, -0.04250993,  0.13152295, ...,  0.10308245,\n",
      "        -0.08123604,  0.05646114],\n",
      "       [ 0.14558454, -0.13749245, -0.05957599, ..., -0.07933711,\n",
      "         0.12258241, -0.08383579],\n",
      "       ...,\n",
      "       [ 0.09188823,  0.13741366,  0.27188086, ...,  0.08381936,\n",
      "        -0.15180401, -0.00396859],\n",
      "       [-0.19813818, -0.13988205,  0.0035366 , ..., -0.19149217,\n",
      "        -0.00095808, -0.1070209 ],\n",
      "       [ 0.08765961,  0.11305135,  0.21979072, ..., -0.02604575,\n",
      "        -0.06398772, -0.10612421]]), 'b': array([[-1.66203954,  0.10457154, -0.98303182, -1.73397415, -2.6234894 ,\n",
      "        -0.1175283 , -1.73561003, -1.41792256, -3.94602472, -1.82123815]])}\n"
     ]
    }
   ],
   "source": [
    "print(svm29.best_estimator_.get_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [3000, 3500, 4000],\n",
    "    'batch_size': [128],\n",
    "    'max_iter': [100, 140],\n",
    "    'epsilon': [1e-6, 1e-5],\n",
    "    'eta': [0.1]\n",
    "}\n",
    "\n",
    "svm18 = get_model(MySVM, 'data/MySVM2.data', param_grid=param_grid, X=X_train1, Y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9708833333333333\n",
      "{'C': 3500, 'batch_size': 128, 'epsilon': 1e-05, 'eta': 0.1, 'max_iter': 140}\n",
      "{'W': array([[-0.11721773,  0.14751221,  0.13017908, ..., -0.3987127 ,\n",
      "         0.08624766,  0.20424375],\n",
      "       [ 0.17146778,  0.1471984 , -0.3955672 , ..., -0.22600087,\n",
      "         0.01635417, -0.22803647],\n",
      "       [ 0.12696431,  0.00186481,  0.0494665 , ...,  0.15406404,\n",
      "         0.07941765, -0.40508291],\n",
      "       ...,\n",
      "       [ 0.28410379, -0.1196908 , -0.21759334, ..., -0.14378999,\n",
      "        -0.01747923, -0.16902245],\n",
      "       [ 0.16176322,  0.10714409,  0.04208303, ..., -0.06643455,\n",
      "        -0.07328502, -0.27009435],\n",
      "       [ 0.03130327,  0.42317872, -0.23258921, ..., -0.11655082,\n",
      "         0.28603592,  0.07569481]]), 'b': array([[-1.44205315,  0.71676242, -0.48244117, -1.53739799, -2.80475429,\n",
      "         0.56763092, -1.59692785, -0.76808621, -3.22254488, -2.0419439 ]])}\n",
      "0.97745\n",
      "0.9738\n"
     ]
    }
   ],
   "source": [
    "print(svm18.best_score_)\n",
    "print(svm18.best_params_)\n",
    "print(svm18.best_estimator_.get_parameters())\n",
    "\n",
    "print(svm18.best_estimator_.score(X_train1, y_train))\n",
    "print(svm18.best_estimator_.score(X_test1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split5_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(svm18.cv_results_).to_csv('MySVM_svm18_train1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W': array([[-0.11721773,  0.14751221,  0.13017908, ..., -0.3987127 ,\n",
      "         0.08624766,  0.20424375],\n",
      "       [ 0.17146778,  0.1471984 , -0.3955672 , ..., -0.22600087,\n",
      "         0.01635417, -0.22803647],\n",
      "       [ 0.12696431,  0.00186481,  0.0494665 , ...,  0.15406404,\n",
      "         0.07941765, -0.40508291],\n",
      "       ...,\n",
      "       [ 0.28410379, -0.1196908 , -0.21759334, ..., -0.14378999,\n",
      "        -0.01747923, -0.16902245],\n",
      "       [ 0.16176322,  0.10714409,  0.04208303, ..., -0.06643455,\n",
      "        -0.07328502, -0.27009435],\n",
      "       [ 0.03130327,  0.42317872, -0.23258921, ..., -0.11655082,\n",
      "         0.28603592,  0.07569481]]), 'b': array([[-1.44205315,  0.71676242, -0.48244117, -1.53739799, -2.80475429,\n",
      "         0.56763092, -1.59692785, -0.76808621, -3.22254488, -2.0419439 ]])}\n"
     ]
    }
   ],
   "source": [
    "print(svm18.best_estimator_.get_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [3400, 3500, 3600],\n",
    "    'batch_size': [128],\n",
    "    'max_iter': [120, 140, 160],\n",
    "    'epsilon': [1e-6, 1e-5, 1e-4],\n",
    "    'eta': [0.1]\n",
    "}\n",
    "\n",
    "svm17 = get_model(MySVM, 'data/MySVM5.data', param_grid=param_grid, X=X_train1, Y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9712666666666666\n",
      "{'C': 3600, 'batch_size': 128, 'epsilon': 1e-06, 'eta': 0.1, 'max_iter': 140}\n",
      "{'W': array([[-0.11151751,  0.2135294 ,  0.20382278, ...,  0.19694961,\n",
      "        -0.15845842, -0.20904872],\n",
      "       [ 0.15784837, -0.15899147, -0.03681736, ..., -0.02858646,\n",
      "        -0.26898294,  0.04704171],\n",
      "       [-0.12895156, -0.06816802, -0.14374408, ..., -0.27802178,\n",
      "        -0.20643542,  0.05486333],\n",
      "       ...,\n",
      "       [-0.11276817, -0.0532217 ,  0.35935393, ..., -0.18056679,\n",
      "         0.08705596,  0.01626816],\n",
      "       [-0.16559266,  0.21541088, -0.24065934, ..., -0.09622628,\n",
      "        -0.28852436, -0.31122237],\n",
      "       [-0.42192208,  0.46062478,  0.17556587, ..., -0.16914525,\n",
      "         0.11211911, -0.22515615]]), 'b': array([[-1.37395103, -0.1101466 , -0.44808261, -1.97124803, -1.90238104,\n",
      "        -0.22335933, -2.50026859, -1.4679542 , -4.21234812, -1.89847586]])}\n",
      "0.9775166666666667\n",
      "0.9728\n"
     ]
    }
   ],
   "source": [
    "print(svm17.best_score_)\n",
    "print(svm17.best_params_)\n",
    "print(svm17.best_estimator_.get_parameters())\n",
    "print(svm17.best_estimator_.score(X_train1, y_train))\n",
    "print(svm17.best_estimator_.score(X_test1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(svm17.cv_results_).to_csv('MySVM_svm17_train1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0: 0.7435422788784037\n",
      "Cost at epoch 48: 0.025371965849831566\n",
      "Cost at epoch 96: 0.022632816324009774\n",
      "Cost at epoch 144: 0.02227781977520921\n",
      "Cost at epoch 192: 0.022211862480572175\n",
      "Cost at epoch 240: 0.022144778804264124\n",
      "Cost at epoch 288: 0.02211765380968727\n",
      "Cost at epoch 336: 0.022079472011284768\n",
      "Cost at epoch 384: 0.022108294652347656\n",
      "Cost at epoch 432: 0.02211495649570549\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MySVM(C=1000, batch_size=256, epsilon=1e-06, eta=0.1, max_iter=480)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm30 = MySVM(C=1000, batch_size=256, epsilon=1e-6, eta=0.1, max_iter=480)\n",
    "svm30.fit(X_train1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9719\n",
      "0.9701\n"
     ]
    }
   ],
   "source": [
    "print(svm30.score(X_train1, y_train))\n",
    "print(svm30.score(X_test1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centerize(X):\n",
    "    X_center = np.zeros(np.shape(X))\n",
    "    mu = np.zeros((1, np.shape(X)[1]))\n",
    "    \n",
    "    mu[0, :] = np.mean(X, axis=0)\n",
    "        \n",
    "    X_center = X - mu\n",
    "    \n",
    "    return X_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVD_like(mat):\n",
    "    \n",
    "    eig_val, eig_vec = np.linalg.eig(mat)\n",
    "    \n",
    "    eig_pair = [(eig_val[i], eig_vec[i]) for i in range(len(eig_val))]\n",
    "    \n",
    "    sorted(eig_pair, key=lambda p: p[0], reverse=True)\n",
    "    sorted_eig_vec = [eig_pair[i][1] for i in range(len(eig_pair))]\n",
    "    \n",
    "    return np.array(sorted_eig_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def principal_component_analysis(X_train, X_test=None, n_components=150):\n",
    "    \n",
    "    X_train_centered = centerize(X_train)\n",
    "    \n",
    "#     eig_vec = SVD_like(np.cov(X_train_centered.T))\n",
    "    \n",
    "#     reduced_eig_vec = eig_vec[:, : n_components]\n",
    "    \n",
    "#     reduced_train = np.matmul(X_train, reduced_eig_vec)\n",
    "#     reduced_test = np.matmul(X_test, reduced_eig_vec)\n",
    "    \n",
    "#     assert(reduced_train.shape == (X_train.shape[0], n_components))\n",
    "    \n",
    "#     return reduced_train, reduced_test\n",
    "\n",
    "    u, s, v = np.linalg.svd(np.cov(X_train_centered.T))\n",
    "    u_reduced = u[:, : n_components]\n",
    "    \n",
    "    reduced_train = np.matmul(X_train, u_reduced)\n",
    "    if X_test is not None:\n",
    "        reduced_test = np.matmul(X_test, u_reduced)\n",
    "        return reduced_train, reduced_test\n",
    "    \n",
    "    return reduced_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_discriminant_analysis(X_train, y_train, X_test, n_components=150):\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    mu = np.zeros((num_classes, np.shape(X_train)[1]))\n",
    "    total_mu = np.zeros((1, np.shape(X_train)[1]))\n",
    "    \n",
    "    Sw = np.zeros((np.shape(X_train)[1], np.shape(X_train)[1]))\n",
    "    Sb = np.zeros((np.shape(X_train)[1], np.shape(X_train)[1]))\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        mu[i, :] = np.mean(X_train[y_train == i], axis=0)\n",
    "    total_mu[0, :] = np.mean(mu, axis=0)\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        temp_w = mu[i, :].reshape(-1, 1) - total_mu.reshape(-1, 1)\n",
    "        Sw += np.matmul(temp_w, temp_w.T)\n",
    "        \n",
    "        temp = X_train[y_train == i].T - mu[i, :].reshape(-1, 1)\n",
    "        temp_b = np.sum(temp, axis=1) / (np.sum(y_train == i) - 1)\n",
    "        Sb += np.matmul(temp_b, temp_b.T)\n",
    "        \n",
    "#     eig_vec = SVD_like(np.matmul(np.linalg.inv(Sw), Sb))\n",
    "    eig_vec, s, v = np.linalg.svd(np.matmul(np.linalg.inv(Sw), Sb))\n",
    "    reduced_eig_vec = eig_vec[:, : n_components]\n",
    "    \n",
    "    reduced_train = np.matmul(X_train, reduced_eig_vec)\n",
    "    reduced_test = np.matmul(X_test, reduced_eig_vec)\n",
    "    \n",
    "    return reduced_train, reduced_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_poly_features2(images_train, labels_train, images_test):\n",
    "    \n",
    "    m_train = images_train.shape[0]\n",
    "    m_test = images_test.shape[0]\n",
    "    \n",
    "    f_train = np.zeros((m_train, 550))\n",
    "    f_test = np.zeros((m_test, 550))\n",
    "    \n",
    "    X_train = np.reshape(images_train, (-1, 784))\n",
    "    X_test = np.reshape(images_test, (-1, 784))\n",
    "    \n",
    "    X_train, X_test = make_poly_features(images_train, images_test)\n",
    "    \n",
    "    f_train[:, : 100], f_test[:, : 100] = principal_component_analysis(images_train.reshape(-1, 784) / 255, images_test.reshape(-1, 784) / 255, n_components=100)\n",
    "#     f_train[:, 100 : 200] = f_train[:, : 100] ** 2\n",
    "#     f_test[:, 100 : 200] = f_test[:, : 100] ** 2\n",
    "#     f_train[:, 200 : 300] = f_train[:, : 100] ** 3\n",
    "#     f_test[:, 200 : 300] = f_test[:, : 100] ** 3\n",
    "    f_train[:, 100 :], f_test[:, 100 :] = linear_discriminant_analysis(X_train, labels_train, X_test, n_components=450)\n",
    "    \n",
    "    return f_train, f_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 300) (10000, 300)\n"
     ]
    }
   ],
   "source": [
    "X_train1_1, X_test1_1 = principal_component_analysis(X_train1, X_test1, n_components=300)\n",
    "print(X_train1_1.shape, X_test1_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 300) (10000, 300)\n"
     ]
    }
   ],
   "source": [
    "X_train1_2, X_test1_2 = linear_discriminant_analysis(X_train1, y_train, X_test1, n_components=300)\n",
    "print(X_train1_2.shape, X_test1_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 300) (10000, 300)\n"
     ]
    }
   ],
   "source": [
    "X_train2_1, X_test2_1 = principal_component_analysis(X_train2, X_test2, n_components=300)\n",
    "print(X_train2_1.shape, X_test2_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 300) (10000, 300)\n"
     ]
    }
   ],
   "source": [
    "X_train2_2, X_test2_2 = linear_discriminant_analysis(X_train2, y_train, X_test2, n_components=300)\n",
    "print(X_train2_2.shape, X_test2_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 550) (10000, 550)\n"
     ]
    }
   ],
   "source": [
    "X_train3, X_test3 = make_poly_features2(images_train, labels_train, images_test)\n",
    "print(X_train3.shape, X_test3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0: 0.6411737075465398\n",
      "Cost at epoch 14: 0.05123676151659059\n",
      "Cost at epoch 28: 0.03468459339797425\n",
      "Cost at epoch 42: 0.02896198244484556\n",
      "Cost at epoch 56: 0.026464435237825296\n",
      "Cost at epoch 70: 0.0251863590123125\n",
      "Cost at epoch 84: 0.024494619136511978\n",
      "Cost at epoch 98: 0.02404701991799214\n",
      "Cost at epoch 112: 0.023648399115464198\n",
      "Cost at epoch 126: 0.023475562961978017\n"
     ]
    }
   ],
   "source": [
    "# svm16 = MySVM(C=800, max_iter=100, batch_size=256, eta=0.1)\n",
    "# svm16.fit(X_train3, y_train)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [1000, 3000, 5000],\n",
    "    'batch_size': [128],\n",
    "    'max_iter': [100, 140],\n",
    "    'epsilon': [1e-6, 1e-5],\n",
    "    'eta': [0.1]\n",
    "}\n",
    "\n",
    "svm19 = get_model(MySVM, 'data/MySVM_pca_2.data', param_grid=param_grid, X=X_train2_1, Y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9612\n",
      "{'C': 5000, 'batch_size': 128, 'epsilon': 1e-05, 'eta': 0.1, 'max_iter': 140}\n",
      "{'W': array([[-0.21390191,  0.46890821, -0.07465975, ...,  0.39427718,\n",
      "         0.22347838,  0.22881381],\n",
      "       [-0.0680663 , -0.50260387, -0.24228425, ...,  0.17769017,\n",
      "        -0.01774631,  0.31676386],\n",
      "       [-0.25198895, -0.07480761, -0.01799079, ...,  0.08800352,\n",
      "        -0.07044982,  0.12783986],\n",
      "       ...,\n",
      "       [-0.00290697,  0.38822286,  0.04721271, ..., -0.20181268,\n",
      "        -0.668893  , -0.16160831],\n",
      "       [-0.45113114,  0.16841665,  0.27420922, ...,  0.74060221,\n",
      "        -0.45420076,  0.14166384],\n",
      "       [ 0.04835054,  0.42510864, -0.51964888, ...,  0.41421658,\n",
      "        -0.12295242, -0.22585462]]), 'b': array([[-1.52612183,  0.39331909, -1.8532421 , -2.71426455, -2.9569455 ,\n",
      "        -0.49953788, -2.06171755, -1.57328481, -3.49586168, -2.54507266]])}\n",
      "0.9692\n",
      "0.9654\n"
     ]
    }
   ],
   "source": [
    "print(svm19.best_score_)\n",
    "print(svm19.best_params_)\n",
    "print(svm19.best_estimator_.get_parameters())\n",
    "\n",
    "print(svm19.best_estimator_.score(X_train2_1, y_train))\n",
    "print(svm19.best_estimator_.score(X_test2_1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split5_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(svm19.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('svm19_pca_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0: 0.48932683295464136\n",
      "Cost at epoch 14: 0.04419559353360848\n",
      "Cost at epoch 28: 0.02880823164261982\n",
      "Cost at epoch 42: 0.023278789820220348\n",
      "Cost at epoch 56: 0.020860258709197253\n",
      "Cost at epoch 70: 0.01963223788157185\n",
      "Cost at epoch 84: 0.01898376376060766\n",
      "Cost at epoch 98: 0.018647284956326685\n",
      "Cost at epoch 112: 0.01849057621098567\n",
      "Cost at epoch 126: 0.018309819412845294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MySVM(C=3000, batch_size=128, epsilon=1e-06, eta=0.1, max_iter=140)"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# svm16 = MySVM(C=800, max_iter=100, batch_size=256, eta=0.1)\n",
    "# svm16.fit(X_train3, y_train)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [1000, 3000, 5000],\n",
    "    'batch_size': [128],\n",
    "    'max_iter': [100, 140],\n",
    "    'epsilon': [1e-6, 1e-5],\n",
    "    'eta': [0.1]\n",
    "}\n",
    "\n",
    "svm18 = get_model(MySVM, 'data/MySVM_pca_1.data', param_grid=param_grid, X=X_train1_1, Y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9643166666666667\n",
      "{'C': 5000, 'batch_size': 128, 'epsilon': 1e-06, 'eta': 0.1, 'max_iter': 140}\n",
      "{'W': array([[ 0.32812412, -0.67099888,  0.00908884, ..., -0.45744519,\n",
      "        -0.31934168, -0.10684607],\n",
      "       [-0.16833774, -0.385851  , -0.32817532, ...,  0.467464  ,\n",
      "         0.05119359,  0.51386786],\n",
      "       [-0.3559464 , -0.06851864, -0.07286901, ...,  0.14226421,\n",
      "        -0.15123767,  0.1769827 ],\n",
      "       ...,\n",
      "       [-0.53217675, -0.01356341,  0.37574779, ..., -0.15770324,\n",
      "        -0.30678296,  0.70073816],\n",
      "       [ 0.32655044, -0.42132501, -0.04976439, ...,  0.30219391,\n",
      "         0.41638435, -0.15553092],\n",
      "       [ 0.09011209,  0.26667697,  0.18750308, ...,  0.26438093,\n",
      "        -0.62708056, -0.41335652]]), 'b': array([[-1.7067485 ,  0.16843686, -1.14272568, -2.81020837, -3.06580705,\n",
      "        -0.44149976, -1.89194673, -1.55026339, -3.08337275, -2.29915054]])}\n",
      "0.9707666666666667\n",
      "0.9675\n"
     ]
    }
   ],
   "source": [
    "print(svm18.best_score_)\n",
    "print(svm18.best_params_)\n",
    "print(svm18.best_estimator_.get_parameters())\n",
    "\n",
    "print(svm18.best_estimator_.score(X_train1_1, y_train))\n",
    "print(svm18.best_estimator_.score(X_test1_1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split5_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\jylee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(svm18.cv_results_)\n",
    "df.to_csv('svm18_pca_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0: 0.5004672507202607\n",
      "Cost at epoch 10: 0.05134592662546951\n",
      "Cost at epoch 20: 0.03343726607192031\n",
      "Cost at epoch 30: 0.0266278815669175\n",
      "Cost at epoch 40: 0.0233338041017626\n",
      "Cost at epoch 50: 0.021649595190985683\n",
      "Cost at epoch 60: 0.02067620977447049\n",
      "Cost at epoch 70: 0.02017142012884437\n",
      "Cost at epoch 80: 0.019821455581828188\n",
      "Cost at epoch 90: 0.019595970609721635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MySVM(C=3500, batch_size=128, epsilon=1e-08, eta=0.1, max_iter=100)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm21 = MySVM(C=3500, max_iter=100, batch_size=128, eta=0.1)\n",
    "svm21.fit(X_train1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.975675\n",
      "0.964\n"
     ]
    }
   ],
   "source": [
    "print(svm21.score(X_train1, y_train))\n",
    "print(svm21.score(X_test1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0: 0.6286434202204312\n",
      "Cost at epoch 14: 0.05016631249522425\n",
      "Cost at epoch 28: 0.031766432269513846\n",
      "Cost at epoch 42: 0.02507985418714191\n",
      "Cost at epoch 56: 0.02199970980531646\n",
      "Cost at epoch 70: 0.020416188305290454\n",
      "Cost at epoch 84: 0.019631569975251766\n",
      "Cost at epoch 98: 0.019167804076441225\n",
      "Cost at epoch 112: 0.018831380530505575\n",
      "Cost at epoch 126: 0.018724202787300467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MySVM(C=3500, batch_size=128, epsilon=1e-08, eta=0.1, max_iter=140)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm22 = MySVM(C=3500, max_iter=140, batch_size=128, eta=0.1)\n",
    "svm22.fit(X_train2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9753833333333334\n",
      "0.9718\n"
     ]
    }
   ],
   "source": [
    "print(svm22.score(X_train2, y_train))\n",
    "print(svm22.score(X_test2, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0: 0.5279850300148776\n",
      "Cost at epoch 14: 0.04469477023314208\n",
      "Cost at epoch 28: 0.029430569284351886\n",
      "Cost at epoch 42: 0.02494266396091238\n",
      "Cost at epoch 56: 0.023346218007564676\n",
      "Cost at epoch 70: 0.022784549209448657\n",
      "Cost at epoch 84: 0.022495127485833245\n",
      "Cost at epoch 98: 0.022404386725391696\n",
      "Cost at epoch 112: 0.022323744190876297\n",
      "Cost at epoch 126: 0.0223217885468603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MySVM(C=3500, batch_size=128, epsilon=1e-08, eta=0.1, max_iter=140)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm23 = MySVM(C=3500, max_iter=140, batch_size=128, eta=0.1)\n",
    "svm23.fit(X_train1_1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9698833333333333\n",
      "0.9683\n"
     ]
    }
   ],
   "source": [
    "print(svm23.score(X_train1_1, y_train))\n",
    "print(svm23.score(X_test1_1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0: 0.6210781295022839\n",
      "Cost at epoch 14: 0.048014652193324035\n",
      "Cost at epoch 28: 0.03186326403404652\n",
      "Cost at epoch 42: 0.027480261131487504\n",
      "Cost at epoch 56: 0.025842778429804005\n",
      "Cost at epoch 70: 0.02538987910184618\n",
      "Cost at epoch 84: 0.025089065914957902\n",
      "Cost at epoch 98: 0.02496420578231034\n",
      "Cost at epoch 112: 0.024940744751733687\n",
      "Cost at epoch 126: 0.02479058452007617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MySVM(C=3500, batch_size=128, epsilon=1e-08, eta=0.1, max_iter=140)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm24 = MySVM(C=3500, max_iter=140, batch_size=128, eta=0.1)\n",
    "svm24.fit(X_train2_1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9678833333333333\n",
      "0.9638\n"
     ]
    }
   ],
   "source": [
    "print(svm24.score(X_train2_1, y_train))\n",
    "print(svm24.score(X_test2_1, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0: 0.5567875355996388\n",
      "Cost at epoch 14: 0.046294531345514534\n",
      "Cost at epoch 28: 0.029704094545581976\n",
      "Cost at epoch 42: 0.023618518156009058\n",
      "Cost at epoch 56: 0.020926683615308788\n",
      "Cost at epoch 70: 0.019542757227408468\n",
      "Cost at epoch 84: 0.018758194536583866\n",
      "Cost at epoch 98: 0.018291841266288522\n",
      "Cost at epoch 112: 0.017999659656631282\n",
      "Cost at epoch 126: 0.017883131581233443\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MySVM(C=3500, batch_size=128, epsilon=1e-08, eta=0.1, max_iter=140)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm25 = MySVM(C=3500, max_iter=140, batch_size=128, eta=0.1)\n",
    "svm25.fit(X_train1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9774166666666667\n",
      "0.9727\n"
     ]
    }
   ],
   "source": [
    "print(svm25.score(X_train1, y_train))\n",
    "print(svm25.score(X_test1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0: 0.6338581500761664\n",
      "Cost at epoch 14: 0.050874106168308196\n",
      "Cost at epoch 28: 0.03214584988938263\n",
      "Cost at epoch 42: 0.025356649337598403\n",
      "Cost at epoch 56: 0.022284647293940276\n",
      "Cost at epoch 70: 0.02066506016669776\n",
      "Cost at epoch 84: 0.01968927535622095\n",
      "Cost at epoch 98: 0.01916811546637401\n",
      "Cost at epoch 112: 0.018837767567501034\n",
      "Cost at epoch 126: 0.018697693767661946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MySVM(C=3500, batch_size=128, epsilon=1e-08, eta=0.1, max_iter=140)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm26 = MySVM(C=3500, max_iter=140, batch_size=128, eta=0.1)\n",
    "svm26.fit(X_train2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9748666666666667\n",
      "0.9693\n"
     ]
    }
   ],
   "source": [
    "print(svm26.score(X_train2, y_train))\n",
    "print(svm26.score(X_test2, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0: 0.5000327333377631\n",
      "Cost at epoch 14: 0.04484374174463349\n",
      "Cost at epoch 28: 0.030086810667382783\n",
      "Cost at epoch 42: 0.02534807354088641\n",
      "Cost at epoch 56: 0.02364159493797155\n",
      "Cost at epoch 70: 0.02288923122558651\n",
      "Cost at epoch 84: 0.02262021094151225\n",
      "Cost at epoch 98: 0.022475709608005018\n",
      "Cost at epoch 112: 0.022399396095785444\n",
      "Cost at epoch 126: 0.022377660304338146\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MySVM(C=3500, batch_size=128, epsilon=1e-08, eta=0.1, max_iter=140)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm27 = MySVM(C=3500, max_iter=140, batch_size=128, eta=0.1)\n",
    "svm27.fit(X_train1_1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9697666666666667\n",
      "0.9675\n"
     ]
    }
   ],
   "source": [
    "print(svm27.score(X_train1_1, y_train))\n",
    "print(svm27.score(X_test1_1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0: 0.6313096979437539\n",
      "Cost at epoch 14: 0.047376759586617356\n",
      "Cost at epoch 28: 0.0319893447928701\n",
      "Cost at epoch 42: 0.027594463977368554\n",
      "Cost at epoch 56: 0.025936456109261655\n",
      "Cost at epoch 70: 0.02521580983213248\n",
      "Cost at epoch 84: 0.025078552246877238\n",
      "Cost at epoch 98: 0.024943205383054005\n",
      "Cost at epoch 112: 0.02502725979881426\n",
      "Cost at epoch 126: 0.024808768563934724\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MySVM(C=3500, batch_size=128, epsilon=1e-08, eta=0.1, max_iter=140)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm28 = MySVM(C=3500, max_iter=140, batch_size=128, eta=0.1)\n",
    "svm28.fit(X_train2_1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9673666666666667\n",
      "0.9654\n"
     ]
    }
   ],
   "source": [
    "print(svm28.score(X_train2_1, y_train))\n",
    "print(svm28.score(X_test2_1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "style = dict(size=15, color='black', zorder=20)\n",
    "\n",
    "sns.barplot(x=['Normal', 'PCA', 'LRD', 'LRD+PCA'], y=[97.35, 96.83, 97.27, 96.75], ax=ax, zorder=10)\n",
    "ax.text(0, 55, '97.35', ha='center', **style)\n",
    "ax.text(1, 55, '96.83', ha='center', **style)\n",
    "ax.text(2, 55, '97.27', ha='center', **style)\n",
    "ax.text(3, 55, '96.75', ha='center', **style)\n",
    "ax.grid(True, axis='y', zorder=0)\n",
    "\n",
    "plt.title('Polynomial Features with Gradient of Images and HoG')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "style = dict(size=15, color='black', zorder=20)\n",
    "\n",
    "sns.barplot(x=['Normal', 'PCA', 'LRD', 'LRD+PCA'], y=[97.18, 96.38, 96.93, 96.54], ax=ax, zorder=10)\n",
    "ax.text(0, 55, '97.18', ha='center', **style)\n",
    "ax.text(1, 55, '96.38', ha='center', **style)\n",
    "ax.text(2, 55, '96.93', ha='center', **style)\n",
    "ax.text(3, 55, '96.75', ha='center', **style)\n",
    "ax.grid(True, axis='y', zorder=0)\n",
    "\n",
    "plt.title('More Complex Polynomial Features with Gradient of Images and HoG')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using NEW data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0: 0.32940399194849806\n",
      "Cost at epoch 20: 0.07332278851076657\n",
      "Cost at epoch 40: 0.07322536171102914\n",
      "Cost at epoch 60: 0.07340241195812529\n",
      "Cost at epoch 80: 0.07316462392616682\n",
      "Cost at epoch 100: 0.0732092184078106\n",
      "Cost at epoch 120: 0.07307172336236248\n",
      "Cost at epoch 140: 0.07305519105185415\n",
      "Cost at epoch 160: 0.07325391808665663\n",
      "Cost at epoch 180: 0.07322504224066118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MySVM(C=10, batch_size=256, epsilon=1e-06, eta=0.1, max_iter=200)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm31 = MySVM(C=10, max_iter=200, batch_size=256, eta=0.1, epsilon=1e-6)\n",
    "svm31.fit(X_train1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8931\n",
      "0.8504\n"
     ]
    }
   ],
   "source": [
    "print(svm31.score(X_train1, y_train))\n",
    "print(svm31.score(X_test1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0: 0.49875536922754554\n",
      "Cost at epoch 14: 0.03682432274994522\n",
      "Cost at epoch 28: 0.02509577029149028\n",
      "Cost at epoch 42: 0.02259840237150455\n",
      "Cost at epoch 56: 0.021824809720563502\n",
      "Cost at epoch 70: 0.021565624179254627\n",
      "Cost at epoch 84: 0.021462587448148152\n",
      "Cost at epoch 98: 0.02138001465017672\n",
      "Cost at epoch 112: 0.021287241444508904\n",
      "Cost at epoch 126: 0.0212716677614172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MySVM(C=2000, batch_size=128, epsilon=1e-06, eta=0.1, max_iter=140)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm32 = MySVM(C=2000, batch_size=128, epsilon=1e-6, eta=0.1, max_iter=140)\n",
    "svm32.fit(X_train1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.974075\n",
      "0.9611\n"
     ]
    }
   ],
   "source": [
    "print(svm32.score(X_train1, y_train))\n",
    "print(svm32.score(X_test1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0: 0.46169277901930755\n",
      "Cost at epoch 12: 0.044314241645463257\n",
      "Cost at epoch 24: 0.029027493930966052\n",
      "Cost at epoch 36: 0.02382230281136949\n",
      "Cost at epoch 48: 0.021682122975913348\n",
      "Cost at epoch 60: 0.020769815721963004\n",
      "Cost at epoch 72: 0.020343427162660407\n",
      "Cost at epoch 84: 0.02007903576596676\n",
      "Cost at epoch 96: 0.019859341339561236\n",
      "Cost at epoch 108: 0.019781534153832873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MySVM(C=3000, batch_size=128, epsilon=1e-06, eta=0.1, max_iter=120)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm33 = MySVM(C=3000, batch_size=128, epsilon=1e-6, eta=0.1, max_iter=120)\n",
    "svm33.fit(X_train1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9751375\n",
      "0.9634\n"
     ]
    }
   ],
   "source": [
    "print(svm33.score(X_train1, y_train))\n",
    "print(svm33.score(X_test1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0: 0.46661067859120386\n",
      "Cost at epoch 12: 0.04575007107003653\n",
      "Cost at epoch 24: 0.030189782790959498\n",
      "Cost at epoch 36: 0.024789191963618642\n",
      "Cost at epoch 48: 0.02237083389387941\n",
      "Cost at epoch 60: 0.021199653905492884\n",
      "Cost at epoch 72: 0.020547962872874165\n",
      "Cost at epoch 84: 0.02017684544487005\n",
      "Cost at epoch 96: 0.01985304061151586\n",
      "Cost at epoch 108: 0.01969957457583046\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C': [2500, 3000, 3500],\n",
    "    'batch_size': [128, 256],\n",
    "    'max_iter': [80, 100, 120],\n",
    "    'epsilon': [1e-6, 1e-5, 1e-4],\n",
    "    'eta': [0.1]\n",
    "}\n",
    "\n",
    "svm34 = get_model(MySVM, 'data/MySVM6.data', param_grid=param_grid, X=X_train1, Y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.969125\n",
      "{'C': 3000, 'batch_size': 128, 'epsilon': 1e-05, 'eta': 0.1, 'max_iter': 120}\n",
      "{'W': array([[-0.16582055,  0.12977394, -0.02721224, ..., -0.06322164,\n",
      "         0.07951149,  0.14110873],\n",
      "       [ 0.12207459, -0.20273143, -0.0151158 , ...,  0.21847416,\n",
      "        -0.0488652 , -0.25587117],\n",
      "       [-0.08006568,  0.01578766,  0.1302908 , ...,  0.05152386,\n",
      "        -0.17710384, -0.20635521],\n",
      "       ...,\n",
      "       [-0.07328833, -0.01133344,  0.01258361, ..., -0.01713797,\n",
      "        -0.20738709, -0.16110956],\n",
      "       [-0.27871563, -0.07699144,  0.38529885, ..., -0.11907301,\n",
      "        -0.06123669, -0.06525174],\n",
      "       [ 0.06123447,  0.2709687 ,  0.07144766, ..., -0.23807059,\n",
      "        -0.05570399, -0.00112006]]), 'b': array([[-1.4007554 ,  0.27122374, -1.04808382, -2.48286222, -2.58446761,\n",
      "        -0.21125334, -1.73920687, -1.0860246 , -3.84951094, -1.52165789]])}\n",
      "0.9757875\n",
      "0.9643\n"
     ]
    }
   ],
   "source": [
    "print(svm34.best_score_)\n",
    "print(svm34.best_params_)\n",
    "print(svm34.best_estimator_.get_parameters())\n",
    "\n",
    "print(svm34.best_estimator_.score(X_train1, y_train))\n",
    "print(svm34.best_estimator_.score(X_test1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [2800, 3000, 3200],\n",
    "    'batch_size': [128],\n",
    "    'max_iter': [110, 120, 130],\n",
    "    'epsilon': [1e-5],\n",
    "    'eta': [0.1]\n",
    "}\n",
    "\n",
    "svm34 = get_model(MySVM, 'data/MySVM7.data', param_grid=param_grid, X=X_train1, Y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(svm34.best_score_)\n",
    "print(svm34.best_params_)\n",
    "print(svm34.best_estimator_.get_parameters())\n",
    "\n",
    "print(svm34.best_estimator_.score(X_train1, y_train))\n",
    "print(svm34.best_estimator_.score(X_test1, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0: 0.488409216743553\n",
      "Cost at epoch 14: 0.040187730037779465\n",
      "Cost at epoch 28: 0.02640411788586003\n",
      "Cost at epoch 42: 0.02267565884221039\n",
      "Cost at epoch 56: 0.021380735829661292\n",
      "Cost at epoch 70: 0.020900626498853506\n",
      "Cost at epoch 84: 0.020721446815663017\n",
      "Cost at epoch 98: 0.020513668595483706\n",
      "Cost at epoch 112: 0.02044506708983655\n",
      "Cost at epoch 126: 0.0203983816494079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MySVM(C=2500, batch_size=128, epsilon=1e-06, eta=0.1, max_iter=140)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm35 = MySVM(C=2500, eta=0.1, batch_size=128, epsilon=1e-6, max_iter=140)\n",
    "svm35.fit(X_train1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9748125\n",
      "0.9632\n"
     ]
    }
   ],
   "source": [
    "# print(svm35.best_score_)\n",
    "# print(svm35.best_params_)\n",
    "# print(svm35.best_estimator_.get_parameters())\n",
    "\n",
    "print(svm35.score(X_train1, y_train))\n",
    "print(svm35.score(X_test1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
